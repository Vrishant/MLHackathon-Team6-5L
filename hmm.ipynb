{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adc726f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and cleaning corpus...\n",
      "Bigram HMM model saved (Initial and Transition log-probabilities).\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "print(\"Loading and cleaning corpus...\")\n",
    "with open('corpus.txt','r') as f:\n",
    "    corpus_words = f.read().splitlines()\n",
    "\n",
    "# Clean words as per original notebook\n",
    "words = [w.strip().lower() for w in corpus_words if re.match('^[a-zA-Z]+$', w)]\n",
    "words = list(set(words))\n",
    "alphabet = list(\"abcdefghijklmnopqrstuvwxyz\")\n",
    "\n",
    "# --- HMM Component Calculation ---\n",
    "\n",
    "# 1. Initial Probabilities (pi)\n",
    "initial_counts = Counter()\n",
    "for word in words:\n",
    "    if len(word) > 0:\n",
    "        initial_counts[word[0]] += 1\n",
    "\n",
    "total_words = len(words)\n",
    "default_initial_log_prob = np.log(1 / (total_words + 27))\n",
    "initial_probs = {char: np.log((initial_counts[char] + 1) / (total_words + 27)) for char in alphabet}\n",
    "initial_probs = defaultdict(lambda: default_initial_log_prob, initial_probs)\n",
    "\n",
    "\n",
    "# 2. Transition Probabilities (A)\n",
    "transition_counts = defaultdict(Counter)\n",
    "for word in words:\n",
    "    for i in range(len(word) - 1):\n",
    "        prev_char = word[i]\n",
    "        next_char = word[i+1]\n",
    "        transition_counts[prev_char][next_char] += 1\n",
    "\n",
    "transition_probs = defaultdict(lambda: defaultdict(lambda: default_initial_log_prob))\n",
    "for prev_char, next_counts in transition_counts.items():\n",
    "    total_transitions = sum(next_counts.values())\n",
    "    for next_char in alphabet:\n",
    "        prob = (next_counts[next_char] + 1) / (total_transitions + 27) \n",
    "        transition_probs[prev_char][next_char] = np.log(prob)\n",
    "\n",
    "# --- Save Model ---\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# !! FIX: Convert defaultdicts to regular dicts for pickling\n",
    "initial_probs_dict = dict(initial_probs)\n",
    "transition_probs_dict = {k: dict(v) for k, v in transition_probs.items()}\n",
    "\n",
    "hmm_model = {\n",
    "    \"initial_probs\": initial_probs_dict,\n",
    "    \"transition_probs\": transition_probs_dict,\n",
    "    \"default_log_prob\": default_initial_log_prob # Save the default value\n",
    "}\n",
    "\n",
    "# Save with a new name to avoid confusion\n",
    "with open('models/bigram_hmm_model.pkl','wb') as f:\n",
    "    pickle.dump(hmm_model, f) # This will now work\n",
    "\n",
    "print(\"Bigram HMM model saved (Initial and Transition log-probabilities).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4849e6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting RL agent training...\n",
      "Episode   500 | WinRate  0.80% | AvgReward  -111.06 | Eps  0.778\n",
      "Episode  1000 | WinRate  1.50% | AvgReward   -95.34 | Eps  0.606\n",
      "Episode  1500 | WinRate  2.53% | AvgReward   -83.08 | Eps  0.472\n",
      "Episode  2000 | WinRate  4.30% | AvgReward   -60.73 | Eps  0.368\n",
      "Episode  2500 | WinRate  6.08% | AvgReward   -43.57 | Eps  0.286\n",
      "Episode  3000 | WinRate  7.53% | AvgReward   -38.40 | Eps  0.223\n",
      "Episode  3500 | WinRate  9.09% | AvgReward   -21.87 | Eps  0.174\n",
      "Episode  4000 | WinRate 10.53% | AvgReward   -14.14 | Eps  0.135\n",
      "Episode  4500 | WinRate 12.02% | AvgReward    -4.48 | Eps  0.105\n",
      "Episode  5000 | WinRate 13.46% | AvgReward     7.03 | Eps  0.082\n",
      "Episode  5500 | WinRate 14.75% | AvgReward    11.87 | Eps  0.064\n",
      "Episode  6000 | WinRate 15.77% | AvgReward     7.61 | Eps  0.050\n",
      "Episode  6500 | WinRate 16.92% | AvgReward    21.77 | Eps  0.039\n",
      "Episode  7000 | WinRate 17.73% | AvgReward    15.54 | Eps  0.030\n",
      "Episode  7500 | WinRate 18.48% | AvgReward    18.26 | Eps  0.023\n",
      "Episode  8000 | WinRate 19.34% | AvgReward    32.31 | Eps  0.018\n",
      "Episode  8500 | WinRate 20.14% | AvgReward    30.96 | Eps  0.014\n",
      "Episode  9000 | WinRate 20.88% | AvgReward    29.04 | Eps  0.011\n",
      "Episode  9500 | WinRate 21.51% | AvgReward    31.20 | Eps  0.010\n",
      "Episode 10000 | WinRate 21.89% | AvgReward    19.81 | Eps  0.010\n",
      "Episode 10500 | WinRate 22.40% | AvgReward    29.24 | Eps  0.010\n",
      "Episode 11000 | WinRate 22.79% | AvgReward    25.16 | Eps  0.010\n",
      "Episode 11500 | WinRate 23.13% | AvgReward    22.86 | Eps  0.010\n",
      "Episode 12000 | WinRate 23.33% | AvgReward    13.00 | Eps  0.010\n",
      "Episode 12500 | WinRate 23.76% | AvgReward    34.73 | Eps  0.010\n",
      "Episode 13000 | WinRate 24.07% | AvgReward    30.62 | Eps  0.010\n",
      "Episode 13500 | WinRate 24.64% | AvgReward    56.07 | Eps  0.010\n",
      "Episode 14000 | WinRate 24.98% | AvgReward    33.59 | Eps  0.010\n",
      "Episode 14500 | WinRate 25.28% | AvgReward    31.50 | Eps  0.010\n",
      "Episode 15000 | WinRate 25.44% | AvgReward    18.20 | Eps  0.010\n",
      "Episode 15500 | WinRate 25.56% | AvgReward    17.91 | Eps  0.010\n",
      "Episode 16000 | WinRate 25.66% | AvgReward    15.68 | Eps  0.010\n",
      "Episode 16500 | WinRate 25.82% | AvgReward    22.54 | Eps  0.010\n",
      "Episode 17000 | WinRate 25.91% | AvgReward    16.74 | Eps  0.010\n",
      "Episode 17500 | WinRate 26.09% | AvgReward    29.77 | Eps  0.010\n",
      "Episode 18000 | WinRate 26.17% | AvgReward    17.41 | Eps  0.010\n",
      "Episode 18500 | WinRate 26.44% | AvgReward    45.63 | Eps  0.010\n",
      "Episode 19000 | WinRate 26.55% | AvgReward    21.73 | Eps  0.010\n",
      "Episode 19500 | WinRate 26.64% | AvgReward    23.04 | Eps  0.010\n",
      "Episode 20000 | WinRate 26.88% | AvgReward    43.86 | Eps  0.010\n",
      "Episode 20500 | WinRate 27.01% | AvgReward    25.22 | Eps  0.010\n",
      "Episode 21000 | WinRate 27.07% | AvgReward    18.95 | Eps  0.010\n",
      "Episode 21500 | WinRate 27.15% | AvgReward    20.85 | Eps  0.010\n",
      "Episode 22000 | WinRate 27.25% | AvgReward    27.94 | Eps  0.010\n",
      "Episode 22500 | WinRate 27.37% | AvgReward    28.39 | Eps  0.010\n",
      "Episode 23000 | WinRate 27.50% | AvgReward    32.52 | Eps  0.010\n",
      "Episode 23500 | WinRate 27.62% | AvgReward    31.67 | Eps  0.010\n",
      "Episode 24000 | WinRate 27.73% | AvgReward    26.30 | Eps  0.010\n",
      "Episode 24500 | WinRate 27.82% | AvgReward    32.36 | Eps  0.010\n",
      "Episode 25000 | WinRate 28.00% | AvgReward    42.73 | Eps  0.010\n",
      "Episode 25500 | WinRate 28.10% | AvgReward    35.00 | Eps  0.010\n",
      "Episode 26000 | WinRate 28.24% | AvgReward    38.24 | Eps  0.010\n",
      "Episode 26500 | WinRate 28.33% | AvgReward    28.98 | Eps  0.010\n",
      "Episode 27000 | WinRate 28.40% | AvgReward    27.28 | Eps  0.010\n",
      "Episode 27500 | WinRate 28.45% | AvgReward    22.76 | Eps  0.010\n",
      "Episode 28000 | WinRate 28.54% | AvgReward    30.54 | Eps  0.010\n",
      "Episode 28500 | WinRate 28.62% | AvgReward    31.24 | Eps  0.010\n",
      "Episode 29000 | WinRate 28.69% | AvgReward    29.74 | Eps  0.010\n",
      "Episode 29500 | WinRate 28.71% | AvgReward    20.40 | Eps  0.010\n",
      "Episode 30000 | WinRate 28.69% | AvgReward    14.43 | Eps  0.010\n",
      "Episode 30500 | WinRate 28.79% | AvgReward    39.49 | Eps  0.010\n",
      "Episode 31000 | WinRate 28.83% | AvgReward    25.94 | Eps  0.010\n",
      "Episode 31500 | WinRate 28.93% | AvgReward    39.24 | Eps  0.010\n",
      "Episode 32000 | WinRate 28.97% | AvgReward    24.88 | Eps  0.010\n",
      "Episode 32500 | WinRate 29.06% | AvgReward    40.62 | Eps  0.010\n",
      "Episode 33000 | WinRate 29.07% | AvgReward    22.72 | Eps  0.010\n",
      "Episode 33500 | WinRate 29.15% | AvgReward    34.06 | Eps  0.010\n",
      "Episode 34000 | WinRate 29.17% | AvgReward    23.91 | Eps  0.010\n",
      "Episode 34500 | WinRate 29.26% | AvgReward    36.99 | Eps  0.010\n",
      "Episode 35000 | WinRate 29.30% | AvgReward    26.37 | Eps  0.010\n",
      "Episode 35500 | WinRate 29.34% | AvgReward    27.09 | Eps  0.010\n",
      "Episode 36000 | WinRate 29.38% | AvgReward    30.86 | Eps  0.010\n",
      "Episode 36500 | WinRate 29.41% | AvgReward    22.53 | Eps  0.010\n",
      "Episode 37000 | WinRate 29.46% | AvgReward    34.22 | Eps  0.010\n",
      "Episode 37500 | WinRate 29.45% | AvgReward    18.72 | Eps  0.010\n",
      "Episode 38000 | WinRate 29.48% | AvgReward    27.22 | Eps  0.010\n",
      "Episode 38500 | WinRate 29.56% | AvgReward    38.23 | Eps  0.010\n",
      "Episode 39000 | WinRate 29.59% | AvgReward    26.02 | Eps  0.010\n",
      "Episode 39500 | WinRate 29.63% | AvgReward    30.36 | Eps  0.010\n",
      "RL agent trained and saved.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# --- Load the NEW HMM Model ---\n",
    "with open('models/bigram_hmm_model.pkl', 'rb') as f:\n",
    "    hmm_model = pickle.load(f)\n",
    "\n",
    "# !! FIX: Re-hydrate the dicts back into defaultdicts\n",
    "default_log_prob = hmm_model['default_log_prob']\n",
    "initial_probs = defaultdict(lambda: default_log_prob, hmm_model['initial_probs'])\n",
    "\n",
    "transition_probs = defaultdict(lambda: defaultdict(lambda: default_log_prob))\n",
    "for k, v in hmm_model['transition_probs'].items():\n",
    "    transition_probs[k] = defaultdict(lambda: default_log_prob, v)\n",
    "\n",
    "alphabet = list(\"abcdefghijklmnopqrstuvwxyz\")\n",
    "\n",
    "# --- Hangman Environment (MODIFIED for 6 lives) ---\n",
    "class HangmanEnv:\n",
    "    def __init__(self, words, max_lives=6): # Changed from 8 to 6\n",
    "        self.words = words\n",
    "        self.max_lives = max_lives\n",
    "    \n",
    "    def reset(self):\n",
    "        self.word = random.choice(self.words)\n",
    "        self.guessed = set()\n",
    "        self.lives = self.max_lives\n",
    "        self.pattern = \"_\" * len(self.word)\n",
    "        return self.pattern\n",
    "    \n",
    "    def step(self, letter):\n",
    "        reward = 0\n",
    "        done = False\n",
    "        if letter in self.guessed:\n",
    "            reward -= 4 \n",
    "        elif letter in self.word:\n",
    "            self.guessed.add(letter)\n",
    "            new_pattern = list(self.pattern)\n",
    "            for i, ch in enumerate(self.word):\n",
    "                if ch == letter:\n",
    "                    new_pattern[i] = letter\n",
    "            diff = new_pattern.count(letter) - self.pattern.count(letter)\n",
    "            self.pattern = \"\".join(new_pattern)\n",
    "            reward += 12 + 4 * diff\n",
    "        else:\n",
    "            self.lives -= 1\n",
    "            self.guessed.add(letter)\n",
    "            reward -= 10 \n",
    "        \n",
    "        if \"_\" not in self.pattern:\n",
    "            reward += 150 \n",
    "            done = True\n",
    "        elif self.lives <= 0:\n",
    "            reward -= 100 \n",
    "            done = True\n",
    "        return self.pattern, reward, done\n",
    "\n",
    "# --- QLearningAgent Class (MODIFIED choose_action) ---\n",
    "class QLearningAgent:\n",
    "    def __init__(self, alpha=0.15, gamma=0.95, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.9995):\n",
    "        self.Q = defaultdict(float)\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        \n",
    "    def get_state(self, pattern, guessed):\n",
    "        return (pattern, \"\".join(sorted(guessed)))\n",
    "\n",
    "    def _get_hmm_scores(self, pattern, available):\n",
    "        scores = {a: 0.0 for a in available}\n",
    "        L = len(pattern)\n",
    "        \n",
    "        for pos, char in enumerate(pattern):\n",
    "            if char == '_':\n",
    "                for a in available:\n",
    "                    if pos == 0:\n",
    "                        scores[a] += initial_probs[a] \n",
    "                    \n",
    "                    if pos > 0 and pattern[pos-1] != '_':\n",
    "                        prev_char = pattern[pos-1]\n",
    "                        scores[a] += transition_probs[prev_char][a] \n",
    "                    \n",
    "                    if pos < L - 1 and pattern[pos+1] != '_':\n",
    "                        next_char = pattern[pos+1]\n",
    "                        scores[a] += transition_probs[a][next_char]\n",
    "        return scores\n",
    "\n",
    "    def choose_action(self, state, pattern, guessed):\n",
    "        available = [a for a in alphabet if a not in guessed]\n",
    "        if not available:\n",
    "            return None\n",
    "        \n",
    "        hmm_scores = self._get_hmm_scores(pattern, available)\n",
    "        \n",
    "        if random.random() < self.epsilon:\n",
    "            return random.choice(available)\n",
    "        \n",
    "        HMM_BOOST_FACTOR = 1.0 \n",
    "        scores = {a: self.Q[(state,a)] + HMM_BOOST_FACTOR * hmm_scores[a] for a in available}\n",
    "        \n",
    "        return max(scores, key=scores.get)\n",
    "\n",
    "    def update(self, state, action, reward, next_state):\n",
    "        max_next = max([self.Q[(next_state,a)] for a in alphabet], default=0)\n",
    "        self.Q[(state,action)] += self.alpha * (reward + self.gamma * max_next - self.Q[(state,action)])\n",
    "\n",
    "# --- Training Loop (Using corpus.txt as required) ---\n",
    "with open('corpus.txt', 'r') as f:\n",
    "    words = f.read().splitlines()\n",
    "\n",
    "env = HangmanEnv(words, max_lives=6) \n",
    "agent = QLearningAgent()\n",
    "episodes = 40000 \n",
    "scores = []\n",
    "wins = 0\n",
    "\n",
    "print(\"Starting RL agent training...\")\n",
    "for ep in range(episodes):\n",
    "    pattern = env.reset()\n",
    "    guessed = set()\n",
    "    total_reward = 0\n",
    "    while True:\n",
    "        state = agent.get_state(pattern, guessed)\n",
    "        action = agent.choose_action(state, pattern, guessed)\n",
    "        if action is None:\n",
    "            break\n",
    "        \n",
    "        next_pattern, reward, done = env.step(action)\n",
    "        next_state = agent.get_state(next_pattern, env.guessed)\n",
    "        agent.update(state, action, reward, next_state) \n",
    "        \n",
    "        total_reward += reward\n",
    "        guessed.add(action)\n",
    "        pattern = next_pattern\n",
    "        \n",
    "        if done:\n",
    "            if \"_\" not in pattern:\n",
    "                wins += 1\n",
    "            break\n",
    "            \n",
    "    agent.epsilon = max(agent.epsilon_min, agent.epsilon * agent.epsilon_decay)\n",
    "    scores.append(total_reward)\n",
    "    \n",
    "    if ep % 500 == 0 and ep > 0:\n",
    "        rate = wins/ep\n",
    "        avg = np.mean(scores[-500:])\n",
    "        print(f\"Episode {ep:5d} | WinRate {rate:6.2%} | AvgReward {avg:8.2f} | Eps {agent.epsilon:6.3f}\")\n",
    "\n",
    "# --- Save the trained agent ---\n",
    "with open('models/rl_agent.pkl','wb') as f:\n",
    "    pickle.dump(agent,f)\n",
    "\n",
    "print(\"RL agent trained and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0cd7fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Hybrid RL Agent on 2000 TEST words...\n",
      "\n",
      "============================================================\n",
      "Hybrid HMM+RL Agent Evaluation Results (2000 games)\n",
      "============================================================\n",
      "  Lives per Game: 6\n",
      "\n",
      "--- PERFORMANCE ---\n",
      "  Success Count:    690/2000\n",
      "  Success Rate:     34.50%\n",
      "  Total Wrong Guesses:  9099\n",
      "  Total Repeated Guesses: 0\n",
      "\n",
      "--- SCORING ---\n",
      "  Success Term: (Success Rate * 2000) = 690.00\n",
      "  Wrong Penalty: (Wrong Guesses * 5)   = 45,495.00\n",
      "  Repeat Penalty: (Repeat Guesses * 2) = 0.00\n",
      "------------------------------------------------------------\n",
      "  Final Score: -44,805.00\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# --- Define utility functions and alphabet ---\n",
    "# These are needed for the pickle files to load correctly\n",
    "def default_dict_factory():\n",
    "    return defaultdict(Counter)\n",
    "\n",
    "def default_dict_dict_factory():\n",
    "    return defaultdict(dict)\n",
    "\n",
    "alphabet = list(\"abcdefghijklmnopqrstuvwxyz\")\n",
    "\n",
    "# --- Define Classes (Must match training) ---\n",
    "# The class definitions must be present for pickle.load() to work.\n",
    "\n",
    "class HangmanEnv:\n",
    "    def __init__(self, words, max_lives=6): # CRITICAL: Must be 6 lives\n",
    "        self.words = words\n",
    "        self.max_lives = max_lives\n",
    "    \n",
    "    def reset(self):\n",
    "        if not self.words:\n",
    "            self.word = \"\"\n",
    "            self.guessed = set()\n",
    "            self.lives = 0\n",
    "            self.pattern = \"\"\n",
    "            return self.pattern\n",
    "        self.word = random.choice(self.words)\n",
    "        self.guessed = set()\n",
    "        self.lives = self.max_lives\n",
    "        self.pattern = \"_\" * len(self.word)\n",
    "        return self.pattern\n",
    "    \n",
    "    # Simplified step for evaluation (no rewards needed)\n",
    "    def step(self, letter):\n",
    "        if letter in self.guessed:\n",
    "            return self.pattern, False, None, True # Done, Success, Repeated\n",
    "        \n",
    "        self.guessed.add(letter)\n",
    "        is_wrong = False\n",
    "        \n",
    "        if letter in self.word:\n",
    "            new_pattern = list(self.pattern)\n",
    "            for i, ch in enumerate(self.word):\n",
    "                if ch == letter:\n",
    "                    new_pattern[i] = letter\n",
    "            self.pattern = \"\".join(new_pattern)\n",
    "        else:\n",
    "            self.lives -= 1\n",
    "            is_wrong = True\n",
    "\n",
    "        if \"_\" not in self.pattern:\n",
    "            return self.pattern, True, True, False  # Done, Success, Not Repeated\n",
    "        if self.lives <= 0:\n",
    "            return self.pattern, True, False, False # Done, Failure, Not Repeated\n",
    "\n",
    "        return self.pattern, False, None, is_wrong\n",
    "\n",
    "class QLearningAgent:\n",
    "    # We only need the structure and the choose_action method for evaluation\n",
    "    def __init__(self, alpha=0, gamma=0, epsilon=0, epsilon_min=0, epsilon_decay=0):\n",
    "        self.Q = defaultdict(float)\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        \n",
    "    def get_state(self, pattern, guessed):\n",
    "        return (pattern, \"\".join(sorted(guessed)))\n",
    "\n",
    "    def _get_hmm_scores(self, pattern, available, initial_probs, transition_probs):\n",
    "        scores = {a: 0.0 for a in available}\n",
    "        L = len(pattern)\n",
    "        for pos, char in enumerate(pattern):\n",
    "            if char == '_':\n",
    "                for a in available:\n",
    "                    if pos == 0:\n",
    "                        scores[a] += initial_probs[a]\n",
    "                    if pos > 0 and pattern[pos-1] != '_':\n",
    "                        scores[a] += transition_probs[pattern[pos-1]][a]\n",
    "                    if pos < L - 1 and pattern[pos+1] != '_':\n",
    "                        scores[a] += transition_probs[a][pattern[pos+1]]\n",
    "        return scores\n",
    "\n",
    "    def choose_action(self, state, pattern, guessed, initial_probs, transition_probs):\n",
    "        available = [a for a in alphabet if a not in guessed]\n",
    "        if not available:\n",
    "            return None\n",
    "        \n",
    "        # Get HMM scores\n",
    "        hmm_scores = self._get_hmm_scores(pattern, available, initial_probs, transition_probs)\n",
    "        \n",
    "        # In evaluation (epsilon=0), we ONLY exploit\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.choice(available) \n",
    "        \n",
    "        HMM_BOOST_FACTOR = 1.0 \n",
    "        scores = {a: self.Q.get((state,a), 0.0) + HMM_BOOST_FACTOR * hmm_scores[a] for a in available}\n",
    "        \n",
    "        return max(scores, key=scores.get)\n",
    "\n",
    "# --- Data Loading ---\n",
    "try:\n",
    "    with open('models/bigram_hmm_model.pkl', 'rb') as f:\n",
    "        hmm_model = pickle.load(f)\n",
    "    with open('models/rl_agent.pkl', 'rb') as f:\n",
    "        rl_agent = pickle.load(f)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Models not found. Ensure Cell 1 and Cell 2 ran successfully.\")\n",
    "    exit()\n",
    "\n",
    "with open('test.txt', 'r') as f:\n",
    "    test_words = [w.strip().lower() for w in f.read().splitlines() if w.strip()]\n",
    "\n",
    "# !! FIX: Re-hydrate the dicts back into defaultdicts\n",
    "default_log_prob = hmm_model['default_log_prob']\n",
    "initial_probs = defaultdict(lambda: default_log_prob, hmm_model['initial_probs'])\n",
    "\n",
    "transition_probs = defaultdict(lambda: defaultdict(lambda: default_log_prob))\n",
    "for k, v in hmm_model['transition_probs'].items():\n",
    "    transition_probs[k] = defaultdict(lambda: default_log_prob, v)\n",
    "\n",
    "# --- Evaluation Loop ---\n",
    "print(f\"Evaluating Hybrid RL Agent on {len(test_words)} TEST words...\")\n",
    "env_test = HangmanEnv(test_words, max_lives=6) # 6 lives\n",
    "games_test = len(test_words)\n",
    "success_count = 0\n",
    "total_wrong_guesses = 0\n",
    "total_repeated_guesses = 0\n",
    "\n",
    "# Set agent to pure exploitation mode\n",
    "rl_agent.epsilon = 0.0\n",
    "\n",
    "for i in range(games_test):\n",
    "    pattern = env_test.reset()\n",
    "    guessed = set()\n",
    "    \n",
    "    while True:\n",
    "        state = rl_agent.get_state(pattern, guessed)\n",
    "        action = rl_agent.choose_action(state, pattern, guessed, initial_probs, transition_probs)\n",
    "\n",
    "        if action is None:\n",
    "            break\n",
    "\n",
    "        next_pattern, done, success, is_wrong_or_repeated = env_test.step(action)\n",
    "\n",
    "        # Scoring logic\n",
    "        if is_wrong_or_repeated and action in guessed:\n",
    "             total_repeated_guesses += 1\n",
    "        elif is_wrong_or_repeated:\n",
    "             total_wrong_guesses += 1\n",
    "\n",
    "        guessed.add(action)\n",
    "        pattern = next_pattern\n",
    "\n",
    "        if done:\n",
    "            if success:\n",
    "                success_count += 1\n",
    "            break\n",
    "\n",
    "# --- Calculate Final Score (as per PDF) ---\n",
    "success_rate = success_count / games_test\n",
    "final_score = (success_rate * 2000) - (total_wrong_guesses * 5) - (total_repeated_guesses * 2)\n",
    "\n",
    "# --- Print Results ---\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"Hybrid HMM+RL Agent Evaluation Results ({games_test} games)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Lives per Game: {env_test.max_lives}\")\n",
    "print(\"\\n--- PERFORMANCE ---\")\n",
    "print(f\"  Success Count:    {success_count}/{games_test}\")\n",
    "print(f\"  Success Rate:     {success_rate:.2%}\")\n",
    "print(f\"  Total Wrong Guesses:  {total_wrong_guesses}\")\n",
    "print(f\"  Total Repeated Guesses: {total_repeated_guesses}\")\n",
    "print(\"\\n--- SCORING ---\")\n",
    "print(f\"  Success Term: (Success Rate * 2000) = {success_rate * 2000:,.2f}\")\n",
    "print(f\"  Wrong Penalty: (Wrong Guesses * 5)   = {total_wrong_guesses * 5:,.2f}\")\n",
    "print(f\"  Repeat Penalty: (Repeat Guesses * 2) = {total_repeated_guesses * 2:,.2f}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"  Final Score: {final_score:,.2f}\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
