{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lX8aj7FzNsH",
        "outputId": "877c3fcc-f630-40ff-a98a-cf00ac1e7d66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hmm saved\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import pickle\n",
        "import os\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "with open('corpus.txt','r') as f:\n",
        "    corpus_words = f.read().splitlines()\n",
        "\n",
        "# Combine corpus and test words for better generalization\n",
        "all_words = corpus_words\n",
        "words = [w.strip().lower() for w in all_words if re.match('^[a-zA-Z]+$', w)]\n",
        "words = list(set(words))\n",
        "alphabet = list(\"abcdefghijklmnopqrstuvwxyz\")\n",
        "\n",
        "def default_dict_factory():\n",
        "    return defaultdict(Counter)\n",
        "\n",
        "def default_dict_dict_factory():\n",
        "    return defaultdict(dict)\n",
        "\n",
        "position_counts = defaultdict(default_dict_factory)\n",
        "length_counts = Counter()\n",
        "\n",
        "for word in words:\n",
        "    L = len(word)\n",
        "    length_counts[L] += 1\n",
        "    for pos, ch in enumerate(word):\n",
        "        position_counts[L][pos][ch] += 1\n",
        "\n",
        "position_probs = defaultdict(default_dict_dict_factory)\n",
        "for L, pos_dict in position_counts.items():\n",
        "    for pos, cnt in pos_dict.items():\n",
        "        total = sum(cnt.values()) + 1e-9\n",
        "        for ch in alphabet:\n",
        "            position_probs[L][pos][ch] = cnt[ch]/total\n",
        "\n",
        "os.makedirs('models', exist_ok=True)\n",
        "hmm_model = {\"position_probs\": position_probs, \"length_counts\": length_counts}\n",
        "\n",
        "with open('models/hmm_model.pkl','wb') as f:\n",
        "    pickle.dump(hmm_model, f)\n",
        "\n",
        "print(\"hmm saved\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "def default_dict_factory():\n",
        "    return defaultdict(Counter)\n",
        "\n",
        "def default_dict_dict_factory():\n",
        "    return defaultdict(dict)\n",
        "\n",
        "with open('models/hmm_model.pkl', 'rb') as f:\n",
        "    hmm_model = pickle.load(f)\n",
        "\n",
        "position_probs = hmm_model['position_probs']\n",
        "alphabet = list(\"abcdefghijklmnopqrstuvwxyz\")\n",
        "\n",
        "class HangmanEnv:\n",
        "    def __init__(self, words, max_lives=8):\n",
        "        self.words = words\n",
        "        self.max_lives = max_lives\n",
        "    def reset(self):\n",
        "        self.word = random.choice(self.words)\n",
        "        self.guessed = set()\n",
        "        self.lives = self.max_lives\n",
        "        self.pattern = \"_\" * len(self.word)\n",
        "        return self.pattern\n",
        "    def step(self, letter):\n",
        "        reward = 0\n",
        "        done = False\n",
        "        if letter in self.guessed:\n",
        "            reward -= 4\n",
        "        elif letter in self.word:\n",
        "            self.guessed.add(letter)\n",
        "            new_pattern = list(self.pattern)\n",
        "            for i, ch in enumerate(self.word):\n",
        "                if ch == letter:\n",
        "                    new_pattern[i] = letter\n",
        "            diff = new_pattern.count(letter) - self.pattern.count(letter)\n",
        "            self.pattern = \"\".join(new_pattern)\n",
        "            reward += 12 + 4 * diff\n",
        "        else:\n",
        "            self.lives -= 1\n",
        "            self.guessed.add(letter)\n",
        "            reward -= 10\n",
        "        if \"_\" not in self.pattern:\n",
        "            reward += 150\n",
        "            done = True\n",
        "        elif self.lives <= 0:\n",
        "            reward -= 100\n",
        "            done = True\n",
        "        return self.pattern, reward, done\n",
        "\n",
        "class QLearningAgent:\n",
        "    def __init__(self, alpha=0.15, gamma=0.95, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.9995):\n",
        "        self.Q = defaultdict(float)\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "    def get_state(self, pattern, guessed):\n",
        "        return (pattern, \"\".join(sorted(guessed)))\n",
        "    def choose_action(self, state, pattern, guessed):\n",
        "        available = [a for a in alphabet if a not in guessed]\n",
        "        if not available:\n",
        "            return None\n",
        "        L = len(pattern)\n",
        "        hmm_probs = {a: 0 for a in available}\n",
        "        if L in position_probs:\n",
        "            for pos, ch in enumerate(pattern):\n",
        "                if ch == \"_\":\n",
        "                    for a in available:\n",
        "                        hmm_probs[a] += position_probs[L][pos].get(a, 0)\n",
        "        # Normalize HMM probabilities\n",
        "        total_hmm = sum(hmm_probs.values()) + 1e-9\n",
        "        hmm_probs = {a: hmm_probs[a]/total_hmm for a in available}\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.choice(available)\n",
        "        # Increased weight for HMM probabilities (10 instead of 5)\n",
        "        scores = {a: self.Q[(state,a)] + 10*hmm_probs.get(a,0) for a in available}\n",
        "        return max(scores, key=scores.get)\n",
        "    def update(self, state, action, reward, next_state):\n",
        "        max_next = max([self.Q[(next_state,a)] for a in alphabet], default=0)\n",
        "        self.Q[(state,action)] += self.alpha * (reward + self.gamma * max_next - self.Q[(state,action)])\n",
        "\n",
        "with open('corpus.txt', 'r') as f:\n",
        "    words = f.read().splitlines()\n",
        "\n",
        "env = HangmanEnv(words)\n",
        "agent = QLearningAgent()\n",
        "episodes = 40000  # Increased from 12000 to 40000 for better learning\n",
        "scores = []\n",
        "wins = 0\n",
        "\n",
        "for ep in range(episodes):\n",
        "    pattern = env.reset()\n",
        "    guessed = set()\n",
        "    total_reward = 0\n",
        "    while True:\n",
        "        state = agent.get_state(pattern, guessed)\n",
        "        action = agent.choose_action(state, pattern, guessed)\n",
        "        if action is None:\n",
        "            break\n",
        "        next_pattern, reward, done = env.step(action)\n",
        "        next_state = agent.get_state(next_pattern, env.guessed)\n",
        "        agent.update(state, action, reward, next_state)\n",
        "        total_reward += reward\n",
        "        guessed.add(action)\n",
        "        pattern = next_pattern\n",
        "        if done:\n",
        "            if \"_\" not in pattern:\n",
        "                wins += 1\n",
        "            break\n",
        "    agent.epsilon = max(agent.epsilon_min, agent.epsilon * agent.epsilon_decay)\n",
        "    scores.append(total_reward)\n",
        "    if ep % 500 == 0 and ep > 0:\n",
        "        rate = wins/ep\n",
        "        avg = np.mean(scores[-500:])\n",
        "        print(f\"Episode {ep:5d} | WinRate {rate:6.2%} | AvgReward {avg:8.2f} | Eps {agent.epsilon:6.3f}\")\n",
        "\n",
        "with open('models/rl_agent.pkl','wb') as f:\n",
        "    pickle.dump(agent,f)\n",
        "\n",
        "print(\"RL agent trained and saved.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJRfeYA0ztBJ",
        "outputId": "2533d9ae-000d-46ea-af83-cf11e7c8ec1e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode   500 | WinRate  2.00% | AvgReward  -109.38 | Eps  0.778\n",
            "Episode  1000 | WinRate  4.10% | AvgReward   -84.17 | Eps  0.606\n",
            "Episode  1500 | WinRate  5.47% | AvgReward   -69.22 | Eps  0.472\n",
            "Episode  2000 | WinRate  7.90% | AvgReward   -44.95 | Eps  0.368\n",
            "Episode  2500 | WinRate  9.72% | AvgReward   -35.69 | Eps  0.286\n",
            "Episode  3000 | WinRate 11.73% | AvgReward   -20.12 | Eps  0.223\n",
            "Episode  3500 | WinRate 13.60% | AvgReward    -6.21 | Eps  0.174\n",
            "Episode  4000 | WinRate 15.55% | AvgReward     5.52 | Eps  0.135\n",
            "Episode  4500 | WinRate 17.24% | AvgReward    14.02 | Eps  0.105\n",
            "Episode  5000 | WinRate 18.76% | AvgReward    16.00 | Eps  0.082\n",
            "Episode  5500 | WinRate 19.73% | AvgReward     7.37 | Eps  0.064\n",
            "Episode  6000 | WinRate 20.95% | AvgReward    23.20 | Eps  0.050\n",
            "Episode  6500 | WinRate 22.11% | AvgReward    29.82 | Eps  0.039\n",
            "Episode  7000 | WinRate 23.13% | AvgReward    32.83 | Eps  0.030\n",
            "Episode  7500 | WinRate 24.12% | AvgReward    36.18 | Eps  0.023\n",
            "Episode  8000 | WinRate 25.00% | AvgReward    36.22 | Eps  0.018\n",
            "Episode  8500 | WinRate 25.94% | AvgReward    46.67 | Eps  0.014\n",
            "Episode  9000 | WinRate 26.70% | AvgReward    40.76 | Eps  0.011\n",
            "Episode  9500 | WinRate 27.43% | AvgReward    47.16 | Eps  0.010\n",
            "Episode 10000 | WinRate 27.94% | AvgReward    35.06 | Eps  0.010\n",
            "Episode 10500 | WinRate 28.35% | AvgReward    29.67 | Eps  0.010\n",
            "Episode 11000 | WinRate 28.93% | AvgReward    47.72 | Eps  0.010\n",
            "Episode 11500 | WinRate 29.41% | AvgReward    41.97 | Eps  0.010\n",
            "Episode 12000 | WinRate 29.86% | AvgReward    43.76 | Eps  0.010\n",
            "Episode 12500 | WinRate 30.29% | AvgReward    43.17 | Eps  0.010\n",
            "Episode 13000 | WinRate 30.62% | AvgReward    39.57 | Eps  0.010\n",
            "Episode 13500 | WinRate 30.96% | AvgReward    42.24 | Eps  0.010\n",
            "Episode 14000 | WinRate 31.21% | AvgReward    35.67 | Eps  0.010\n",
            "Episode 14500 | WinRate 31.48% | AvgReward    36.34 | Eps  0.010\n",
            "Episode 15000 | WinRate 31.75% | AvgReward    44.00 | Eps  0.010\n",
            "Episode 15500 | WinRate 31.93% | AvgReward    30.88 | Eps  0.010\n",
            "Episode 16000 | WinRate 32.06% | AvgReward    32.90 | Eps  0.010\n",
            "Episode 16500 | WinRate 32.28% | AvgReward    41.78 | Eps  0.010\n",
            "Episode 17000 | WinRate 32.56% | AvgReward    48.46 | Eps  0.010\n",
            "Episode 17500 | WinRate 32.70% | AvgReward    35.66 | Eps  0.010\n",
            "Episode 18000 | WinRate 32.73% | AvgReward    24.22 | Eps  0.010\n",
            "Episode 18500 | WinRate 33.03% | AvgReward    52.23 | Eps  0.010\n",
            "Episode 19000 | WinRate 33.30% | AvgReward    54.60 | Eps  0.010\n",
            "Episode 19500 | WinRate 33.43% | AvgReward    37.98 | Eps  0.010\n",
            "Episode 20000 | WinRate 33.51% | AvgReward    29.97 | Eps  0.010\n",
            "Episode 20500 | WinRate 33.58% | AvgReward    31.53 | Eps  0.010\n",
            "Episode 21000 | WinRate 33.70% | AvgReward    37.81 | Eps  0.010\n",
            "Episode 21500 | WinRate 33.78% | AvgReward    35.05 | Eps  0.010\n",
            "Episode 22000 | WinRate 33.94% | AvgReward    46.20 | Eps  0.010\n",
            "Episode 22500 | WinRate 34.00% | AvgReward    30.98 | Eps  0.010\n",
            "Episode 23000 | WinRate 34.13% | AvgReward    43.06 | Eps  0.010\n",
            "Episode 23500 | WinRate 34.30% | AvgReward    49.36 | Eps  0.010\n",
            "Episode 24000 | WinRate 34.35% | AvgReward    35.28 | Eps  0.010\n",
            "Episode 24500 | WinRate 34.46% | AvgReward    41.13 | Eps  0.010\n",
            "Episode 25000 | WinRate 34.54% | AvgReward    38.71 | Eps  0.010\n",
            "Episode 25500 | WinRate 34.66% | AvgReward    45.53 | Eps  0.010\n",
            "Episode 26000 | WinRate 34.75% | AvgReward    41.73 | Eps  0.010\n",
            "Episode 26500 | WinRate 34.90% | AvgReward    52.32 | Eps  0.010\n",
            "Episode 27000 | WinRate 35.01% | AvgReward    45.05 | Eps  0.010\n",
            "Episode 27500 | WinRate 35.06% | AvgReward    36.14 | Eps  0.010\n",
            "Episode 28000 | WinRate 35.10% | AvgReward    37.28 | Eps  0.010\n",
            "Episode 28500 | WinRate 35.21% | AvgReward    47.84 | Eps  0.010\n",
            "Episode 29000 | WinRate 35.30% | AvgReward    44.36 | Eps  0.010\n",
            "Episode 29500 | WinRate 35.41% | AvgReward    49.13 | Eps  0.010\n",
            "Episode 30000 | WinRate 35.58% | AvgReward    61.32 | Eps  0.010\n",
            "Episode 30500 | WinRate 35.67% | AvgReward    47.00 | Eps  0.010\n",
            "Episode 31000 | WinRate 35.74% | AvgReward    40.80 | Eps  0.010\n",
            "Episode 31500 | WinRate 35.83% | AvgReward    46.84 | Eps  0.010\n",
            "Episode 32000 | WinRate 35.87% | AvgReward    40.14 | Eps  0.010\n",
            "Episode 32500 | WinRate 35.93% | AvgReward    40.72 | Eps  0.010\n",
            "Episode 33000 | WinRate 35.99% | AvgReward    44.49 | Eps  0.010\n",
            "Episode 33500 | WinRate 36.05% | AvgReward    43.55 | Eps  0.010\n",
            "Episode 34000 | WinRate 36.14% | AvgReward    46.62 | Eps  0.010\n",
            "Episode 34500 | WinRate 36.27% | AvgReward    61.54 | Eps  0.010\n",
            "Episode 35000 | WinRate 36.33% | AvgReward    46.87 | Eps  0.010\n",
            "Episode 35500 | WinRate 36.31% | AvgReward    24.69 | Eps  0.010\n",
            "Episode 36000 | WinRate 36.40% | AvgReward    51.28 | Eps  0.010\n",
            "Episode 36500 | WinRate 36.43% | AvgReward    36.55 | Eps  0.010\n",
            "Episode 37000 | WinRate 36.49% | AvgReward    44.47 | Eps  0.010\n",
            "Episode 37500 | WinRate 36.54% | AvgReward    45.62 | Eps  0.010\n",
            "Episode 38000 | WinRate 36.58% | AvgReward    41.72 | Eps  0.010\n",
            "Episode 38500 | WinRate 36.63% | AvgReward    43.80 | Eps  0.010\n",
            "Episode 39000 | WinRate 36.70% | AvgReward    51.79 | Eps  0.010\n",
            "Episode 39500 | WinRate 36.77% | AvgReward    47.78 | Eps  0.010\n",
            "RL agent trained and saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "# Define the utility functions and alphabet again for safe copy-pasting\n",
        "def default_dict_factory():\n",
        "    return defaultdict(Counter)\n",
        "\n",
        "def default_dict_dict_factory():\n",
        "    return defaultdict(dict)\n",
        "\n",
        "alphabet = list(\"abcdefghijklmnopqrstuvwxyz\")\n",
        "\n",
        "# --- Agent Class (for loading the trained agent) ---\n",
        "# NOTE: This class must match the structure of the one used in Cell 2 for pickle loading to work.\n",
        "class QLearningAgent:\n",
        "    def __init__(self, alpha=0.15, gamma=0.95, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.9995):\n",
        "        self.Q = defaultdict(float)\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "    def get_state(self, pattern, guessed):\n",
        "        return (pattern, \"\".join(sorted(guessed)))\n",
        "\n",
        "    # Re-implement the hybrid action choice from Cell 2 for correct evaluation logic\n",
        "    def choose_action(self, state, pattern, guessed, position_probs):\n",
        "        available = [a for a in alphabet if a not in guessed]\n",
        "        if not available:\n",
        "            return None\n",
        "\n",
        "        L = len(pattern)\n",
        "        hmm_probs = {a: 0 for a in available}\n",
        "        if L in position_probs:\n",
        "            for pos, ch in enumerate(pattern):\n",
        "                if ch == \"_\":\n",
        "                    # Sum up position probabilities for each available letter\n",
        "                    for a in available:\n",
        "                        hmm_probs[a] += position_probs[L][pos].get(a, 0)\n",
        "\n",
        "        # Normalize HMM probabilities (This is often done in Cell 2's training)\n",
        "        total_hmm = sum(hmm_probs.values()) + 1e-9\n",
        "        hmm_probs = {a: hmm_probs[a]/total_hmm for a in available}\n",
        "\n",
        "        # In evaluation mode (epsilon=0), this random block is skipped\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.choice(available)\n",
        "\n",
        "        # Action selection: Q-value + HMM boost (use the same factor as training)\n",
        "        # Assuming factor was 10 in Cell 2 (you used 5 in a temporary class in Cell 3)\n",
        "        # Use a factor of 8 for a balanced approach.\n",
        "        HMM_BOOST_FACTOR = 8\n",
        "        scores = {a: self.Q.get((state,a), 0.0) + HMM_BOOST_FACTOR * hmm_probs.get(a,0) for a in available}\n",
        "        return max(scores, key=scores.get)\n",
        "\n",
        "# --- Hangman Environment (Must match the one used in Cell 2's training) ---\n",
        "class HangmanEnv:\n",
        "    def __init__(self, words, max_lives=8):\n",
        "        self.words = words\n",
        "        self.max_lives = max_lives\n",
        "    def reset(self):\n",
        "        # Handle empty word list gracefully\n",
        "        if not self.words:\n",
        "            self.word = \"\"\n",
        "            self.guessed = set()\n",
        "            self.lives = 0\n",
        "            self.pattern = \"\"\n",
        "            return self.pattern\n",
        "        self.word = random.choice(self.words)\n",
        "        self.guessed = set()\n",
        "        self.lives = self.max_lives\n",
        "        self.pattern = \"_\" * len(self.word)\n",
        "        return self.pattern\n",
        "    # NOTE: The step method is simplified for evaluation to track only done/success\n",
        "    # The reward calculation is only relevant during training.\n",
        "    def step(self, letter):\n",
        "        # The agent should not pass repeated letters, but if it does:\n",
        "        if letter in self.guessed:\n",
        "            return self.pattern, 0, False, None\n",
        "\n",
        "        self.guessed.add(letter)\n",
        "        if letter in self.word:\n",
        "            new_pattern = list(self.pattern)\n",
        "            for i, ch in enumerate(self.word):\n",
        "                if ch == letter:\n",
        "                    new_pattern[i] = letter\n",
        "            self.pattern = \"\".join(new_pattern)\n",
        "        else:\n",
        "            self.lives -= 1\n",
        "\n",
        "        if \"_\" not in self.pattern:\n",
        "            return self.pattern, 0, True, True  # Done, Success\n",
        "        if self.lives <= 0:\n",
        "            return self.pattern, 0, True, False # Done, Failure\n",
        "\n",
        "        return self.pattern, 0, False, None # Not Done\n",
        "\n",
        "# --- Data Loading ---\n",
        "try:\n",
        "    with open('models/hmm_model.pkl', 'rb') as f:\n",
        "        hmm_model = pickle.load(f)\n",
        "    with open('models/rl_agent.pkl', 'rb') as f:\n",
        "        # Load the trained agent into the correct class structure\n",
        "        rl_agent = pickle.load(f)\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: Models not found. Ensure 'models/hmm_model.pkl' and 'models/rl_agent.pkl' were created by previous cells.\")\n",
        "    exit()\n",
        "\n",
        "with open('corpus.txt', 'r') as f:\n",
        "    corpus_words = [w.strip().lower() for w in f.read().splitlines() if w.strip()]\n",
        "\n",
        "with open('test.txt', 'r') as f:\n",
        "    test_words = [w.strip().lower() for w in f.read().splitlines() if w.strip()]\n",
        "\n",
        "# Corpus-specific letter frequencies (optional, not used in the final agent decision)\n",
        "corpus_letter_order = \"eaiorntslcupmdhygbfvkwzxqj\"\n",
        "corpus_letter_priority = {ch: (27 - i) / 27 for i, ch in enumerate(corpus_letter_order)}\n",
        "position_probs = hmm_model['position_probs']\n",
        "\n",
        "\n",
        "# --- RESTRICTED ORACLE (The original get_hmm_probs is removed/simplified) ---\n",
        "# The agent's choose_action method handles the necessary probability lookup now.\n",
        "\n",
        "# --- Evaluation Loop: Correctly Using the Hybrid RL Agent ---\n",
        "\n",
        "# Evaluate on test words\n",
        "print(\"Evaluating Hybrid RL Agent on TEST words...\")\n",
        "env_test = HangmanEnv(test_words)\n",
        "games_test = len(test_words)\n",
        "success_count_test = wrong_guesses_test = repeated_guesses_test = 0\n",
        "\n",
        "# Set agent to pure exploitation mode (epsilon = 0)\n",
        "rl_agent.epsilon = 0.0\n",
        "\n",
        "for _ in range(games_test):\n",
        "    pattern = env_test.reset()\n",
        "    guessed = set()\n",
        "    while True:\n",
        "        state = rl_agent.get_state(pattern, guessed)\n",
        "\n",
        "        # Use the hybrid RL agent to choose the action\n",
        "        action = rl_agent.choose_action(state, pattern, guessed, position_probs)\n",
        "\n",
        "        if action is None:\n",
        "            break\n",
        "\n",
        "        # Check for repeated guess BEFORE the step (agent should not do this, but for robustness)\n",
        "        is_repeated = action in guessed\n",
        "\n",
        "        next_pattern, _, done, success = env_test.step(action)\n",
        "\n",
        "        # Scoring logic\n",
        "        if is_repeated:\n",
        "            repeated_guesses_test += 1\n",
        "        elif action not in env_test.word:\n",
        "            wrong_guesses_test += 1\n",
        "\n",
        "        guessed.add(action)\n",
        "        pattern = next_pattern\n",
        "\n",
        "        if done:\n",
        "            if success:\n",
        "                success_count_test += 1\n",
        "            break\n",
        "\n",
        "# Evaluate on corpus words (sample for speed, matching the previous run)\n",
        "print(\"Evaluating Hybrid RL Agent on CORPUS words (Sample)...\")\n",
        "corpus_sample = corpus_words[:1000]\n",
        "env_corpus = HangmanEnv(corpus_sample)\n",
        "games_corpus = len(corpus_sample)\n",
        "success_count_corpus = wrong_guesses_corpus = repeated_guesses_corpus = 0\n",
        "\n",
        "for _ in range(games_corpus):\n",
        "    pattern = env_corpus.reset()\n",
        "    guessed = set()\n",
        "    while True:\n",
        "        state = rl_agent.get_state(pattern, guessed)\n",
        "\n",
        "        # Use the hybrid RL agent to choose the action\n",
        "        action = rl_agent.choose_action(state, pattern, guessed, position_probs)\n",
        "\n",
        "        if action is None:\n",
        "            break\n",
        "\n",
        "        is_repeated = action in guessed\n",
        "\n",
        "        next_pattern, _, done, success = env_corpus.step(action)\n",
        "\n",
        "        # Scoring logic\n",
        "        if is_repeated:\n",
        "            repeated_guesses_corpus += 1\n",
        "        elif action not in env_corpus.word:\n",
        "            wrong_guesses_corpus += 1\n",
        "\n",
        "        guessed.add(action)\n",
        "        pattern = next_pattern\n",
        "\n",
        "        if done:\n",
        "            if success:\n",
        "                success_count_corpus += 1\n",
        "            break\n",
        "\n",
        "# --- Calculate Final Score ---\n",
        "total_success = success_count_test + success_count_corpus\n",
        "total_games = games_test + games_corpus\n",
        "total_wrong = wrong_guesses_test + wrong_guesses_corpus\n",
        "total_repeated = repeated_guesses_test + repeated_guesses_corpus\n",
        "\n",
        "final_score = (total_success * 2000) - (total_wrong * 5) - (total_repeated * 2)\n",
        "success_rate = total_success / total_games\n",
        "\n",
        "# --- Print Results ---\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Hybrid RL Agent Evaluation Results\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nTEST Words ({games_test} games):\")\n",
        "print(f\"  Success Rate: {success_count_test/games_test:.2%} ({success_count_test}/{games_test})\")\n",
        "print(f\"  Wrong Guesses: {wrong_guesses_test}\")\n",
        "print(f\"  Repeated Guesses: {repeated_guesses_test}\")\n",
        "\n",
        "print(f\"\\nCORPUS Words Sample ({games_corpus} games):\")\n",
        "print(f\"  Success Rate: {success_count_corpus/games_corpus:.2%} ({success_count_corpus}/{games_corpus})\")\n",
        "print(f\"  Wrong Guesses: {wrong_guesses_corpus}\")\n",
        "print(f\"  Repeated Guesses: {repeated_guesses_corpus}\")\n",
        "\n",
        "print(f\"\\nOVERALL RESULTS:\")\n",
        "print(f\"  Total Games: {total_games}\")\n",
        "print(f\"  Success Rate: {success_rate:.2%} ({total_success}/{total_games})\")\n",
        "print(f\"  Total Wrong Guesses: {total_wrong}\")\n",
        "print(f\"  Total Repeated Guesses: {total_repeated}\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"  Final Score: {final_score}\")\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TPrFPVUzzAg",
        "outputId": "57e80f97-b3a9-4537-9b73-b4aac2dab030"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating Hybrid RL Agent on TEST words...\n",
            "Evaluating Hybrid RL Agent on CORPUS words (Sample)...\n",
            "\n",
            "============================================================\n",
            "Hybrid RL Agent Evaluation Results\n",
            "============================================================\n",
            "\n",
            "TEST Words (2000 games):\n",
            "  Success Rate: 38.15% (763/2000)\n",
            "  Wrong Guesses: 13895\n",
            "  Repeated Guesses: 0\n",
            "\n",
            "CORPUS Words Sample (1000 games):\n",
            "  Success Rate: 42.70% (427/1000)\n",
            "  Wrong Guesses: 6684\n",
            "  Repeated Guesses: 0\n",
            "\n",
            "OVERALL RESULTS:\n",
            "  Total Games: 3000\n",
            "  Success Rate: 39.67% (1190/3000)\n",
            "  Total Wrong Guesses: 20579\n",
            "  Total Repeated Guesses: 0\n",
            "------------------------------------------------------------\n",
            "  Final Score: 2277105\n",
            "============================================================\n"
          ]
        }
      ]
    }
  ]
}